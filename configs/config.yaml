# Model Configuration
model:
  d_model: 512 # Increased from 256 (standard from paper)
  n_heads: 8 # Increased from 4 (standard from paper)
  n_layers: 6 # Increased from 3 (standard from paper)
  d_ff: 2048 # Increased from 1024 (standard from paper)
  max_seq_length: 100 # Keep same
  dropout: 0.1 # Keep same
  vocab_size: 37000 # Increased from 10000 (better coverage)

# Training Configuration
training:
  batch_size: 32 # Keep same (works well with T4 GPU)
  num_epochs: 50 # Increased from 30 (more training)
  learning_rate: 0.0001 # Keep same
  warmup_steps: 4000 # Keep same
  gradient_accumulation_steps: 4 # Keep same
  gradient_clip_val: 1.0 # Keep same
  label_smoothing: 0.1 # Keep same
  checkpoint_interval: 5 # Keep same
  early_stopping_patience: 10 # Changed from 100 (stop if stuck)

# Data Configuration
data:
  dataset: "multi30k"
  source_lang: "de"
  target_lang: "en"
  train_size: 29000
  valid_size: 1000
  test_size: 1000
  max_length: 100

# Paths
paths:
  data_dir: "data"
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  cache_dir: "cache"

# Inference Configuration
inference:
  beam_size: 4
  length_penalty: 0.6
  temperature: 1.0

# API Configuration
api:
  host: "0.0.0.0"
  port: 8000
  max_request_size: 1048576
  rate_limit: 100
